# Batch Jobs

This module contains scheduled data pipelines responsible for transforming and moving data across storage and serving layers.

It orchestrates the transition from **Bronze â†’ Silver â†’ Gold** and integrates analytical and serving systems.

---

## ğŸ§± Processing Overview

There are **4 main tasks** executed in this module:

### 1ï¸âƒ£ Bronze â†’ Silver (Delta Lake)

- Parse full schema  
- Detect changes  
- Deduplicate records using timestamp  

### 2ï¸âƒ£ Silver â†’ ClickHouse  

- Transform and load structured data into analytical tables  

### 3ï¸âƒ£ ClickHouse â†’ Neo4j  

- Build graph relationships for entity connections  

### 4ï¸âƒ£ ClickHouse â†’ Pinecone  

- Generate and store embeddings for semantic retrieval  

---

## ğŸ“‚ Module Structure

```text
batch_jobs/
â”œâ”€â”€ config/        # Pipeline & environment configuration
â”œâ”€â”€ dags/          # Airflow DAG definitions
â”œâ”€â”€ io/            # Database & storage readers/writers
â”œâ”€â”€ pipelines/     # Main pipeline entrypoints
â”œâ”€â”€ run_time/      # Runtime helpers & context
â”œâ”€â”€ schema/        # Full bronze-layer schemas
â”œâ”€â”€ script/        # Setup scripts (e.g., create ClickHouse tables)
â”œâ”€â”€ transforms/    # Data transformation logic
â”œâ”€â”€ __init__.py
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

---

## âš™ï¸ Components

### 1ï¸âƒ£ IO

Contains abstractions for interacting with external systems.

Examples:

- Delta Lake reader  
- ClickHouse writer  
- Neo4j writer  
- Pinecone client  

This layer isolates storage logic from pipeline logic.

---

### 2ï¸âƒ£ Schema

Defines the **full schema** of records stored in the Bronze layer.

Unlike the stream processor (which only parses critical fields),  
batch jobs operate on complete records for correctness and consistency.

---

### 3ï¸âƒ£ Transforms

Responsible for transforming data between source and destination.

Position in flow:

Reader â†’ Transform â†’ Writer

Typical operations:

- Change detection  
- Deduplication  
- Field normalization  
- Aggregation  

---

### 4ï¸âƒ£ Script

Contains setup utilities required before running pipelines.

Example:

Create ClickHouse tables before loading data.

---

### 5ï¸âƒ£ Pipelines

Contains the main functions invoked by Airflow.

Responsibilities:

- Load configuration  
- Initialize IO clients  
- Execute transforms  
- Trigger write operations  

Example entrypoints:

- Bronze â†’ Silver  
- Silver â†’ ClickHouse  
- ClickHouse â†’ Neo4j  
- ClickHouse â†’ Pinecone  

---

### 6ï¸âƒ£ DAGs

Airflow orchestration layer.

These DAGs must be mounted into the Airflow `dags_folder` to be detected and executed.

Two types of DAGs:

1ï¸âƒ£ Standard DAG â†’ for local or VM deployments  
2ï¸âƒ£ KubernetesPodOperator DAG â†’ for running jobs on Kubernetes  

---

## ğŸ§  Execution Flow

Bronze Delta â†’ Dedup & Change Detection â†’ Silver Delta  
â†’ Transform â†’ ClickHouse  
â†’ Neo4j + Pinecone

---

## ğŸš€ Quick Start

For quick testing without Airflow, you can run pipelines manually in order:

```bash
python -m batch_jobs.pipelines.bronze_silver.minio_to_minio
python -m batch_jobs.pipelines.silver_silver.minio_to_clickhouse
python -m batch_jobs.pipelines.silver_gold.clickhouse_to_neo4j
python -m batch_jobs.pipelines.silver_gold.clickhouse_to_pinecone
```

Execution order:

1ï¸âƒ£ Dedup & change detection  
2ï¸âƒ£ Load to ClickHouse  
3ï¸âƒ£ Write to Neo4j  
4ï¸âƒ£ Write to Pinecone  

---

## ğŸ’¡ Notes

- Designed for scheduled execution (Airflow)  
- Supports both local and Kubernetes deployments  
- Ensures data consistency across analytical and serving layers  
- Works together with Stream Processor outputs  

---