# Ingestion

This module simulates a streaming ingestion pipeline that reads raw data produced by the Collector, enriches it, and publishes events to Kafka.

The goal is to mimic real-time data flow where different data types and update states are processed together.

---

## ğŸ“‚ Module Structure

```text
ingestion/
â”œâ”€â”€ config/              # Kafka & pipeline configs
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ movie/           # Example structure (expanded)
â”‚   â”‚   â”œâ”€â”€ new/
â”‚   â”‚   â”œâ”€â”€ change/
â”‚   â”‚   â””â”€â”€ old/
â”‚   â”œâ”€â”€ tv_series/       # Same structure as movie
â”‚   â””â”€â”€ person/          # Same structure as movie
â”œâ”€â”€ loader/              # Load & mix data into a simulated stream
â”œâ”€â”€ preprocessor/        # Data enrichment & normalization
â”œâ”€â”€ producer/            # Kafka producer logic
â”œâ”€â”€ __init__.py
â”œâ”€â”€ main.py              # Entry point
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

---

## ğŸŒŠ Streaming Concept

Instead of sending a single batch, the pipeline mixes records from:

- Data Types: `movie`, `tv_series`, `person`
- Data Labels: `new`, `change`, `old`

This simulates how real systems receive heterogeneous updates continuously.

---

## âš™ï¸ Components

### 1ï¸âƒ£ Loader

#### Responsible for reading raw JSONL files from the `data/` directory.

**Behavior**:

- Iterates across all **data types**
- Randomly mixes records from **new / change / old**
- Emits a unified stream of events

**Purpose**: simulate a real event stream instead of static batch input.

---

### 2ï¸âƒ£ Preprocessor

#### Enriches and standardizes events before publishing.

**Typical transformations**:

- Add ingestion metadata
- Update processing timestamp (`timestamp`)
- Minor cleaning / validation

---

### 3ï¸âƒ£ Producer

#### Publishes processed events to Kafka topics.

**Responsibilities**:

- Serialize events (JSON)
- Assign event key (tmdb_id)
- Send to topic based on data type
- Handle delivery callbacks / retries


---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Prepare Environment

Update configs in `config/` (Kafka bootstrap servers, topics).

### 2ï¸âƒ£ Run Pipeline

#### Run the from the root directory:

```bash
export PYTHONPATH=$(pwd)/src
python -m ingestion.main
```

The pipeline will:

1. Load mixed records  
2. Enrich events  
3. Produce messages to Kafka  

---



