# Stream Processing

This module consumes streaming events from Kafka, processes them, and writes structured data into Delta tables stored on MinIO.

A key design principle is **fault tolerance**: records that fail parsing will NOT stop the pipeline.  
Instead, they are redirected to a **Dead Letter Queue (DLQ)** stored in Delta on MinIO, ensuring no data loss.

---

## ğŸ§± High-Level Flow

Kafka â†’ Source â†’ Processor â†’ Delta Sink (MinIO)

If parsing fails â†’ DLQ Delta Table (MinIO)

---

## ğŸ“‚ Module Structure

```text
stream_processor/
â”œâ”€â”€ config/        # Runtime & pipeline configuration
â”œâ”€â”€ processor/     # Core transformation logic
â”œâ”€â”€ runtime/       # Client wrappers (MinIO, configs, shared context)
â”œâ”€â”€ schema/        # Lightweight schemas for parsing
â”œâ”€â”€ sinks/         # Output writers (Delta on MinIO)
â”œâ”€â”€ sources/       # Kafka ingestion logic
â”œâ”€â”€ __init__.py
â”œâ”€â”€ main.py        # Entry point
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

---

## âš™ï¸ Core Concepts

### âœ… Delta Lake on MinIO

Processed records are stored as Delta tables on MinIO, enabling:

- ACID transactions  
- Schema evolution  
- Efficient batch & analytics queries  

---

### âš ï¸ Dead Letter Queue (DLQ)

Streaming systems must never stop because of bad records.

If an event:

- Fails schema parsing  
- Has missing required fields  
- Contains corrupted JSON  

â¡ï¸ It will be written to a **DLQ Delta table** on MinIO instead of breaking the pipeline.

This guarantees:

- No data loss  
- Easier debugging & replay  
- Stable long-running streaming jobs  

---

## ğŸ§© Components

### 1ï¸âƒ£ Sources

Handles Kafka consumption.

Responsibilities:

- Subscribe to configured topics  
- Deserialize messages  
- Convert Kafka records â†’ internal event format  

---

### 2ï¸âƒ£ Schema

Defines lightweight schemas used to parse incoming events.

Design choice:

- Only critical fields are parsed  
- Avoid strict full-schema enforcement  
- Improves performance and resilience  

This is important because streaming payloads may evolve over time.

---

### 3ï¸âƒ£ Processor

Transforms events before writing.

Typical tasks:

- Apply schema parsing  
- Normalize fields  
- Add ingestion metadata  
- Route invalid events â†’ DLQ  

---

### 4ï¸âƒ£ Runtime

Provides shared infrastructure clients.

Examples:

- MinIO client wrapper  
- Config loader  
- Shared Spark / streaming context (if applicable)  

This layer isolates external dependencies from business logic.

---

### 5ï¸âƒ£ Sinks

Responsible for writing output data.

Outputs:

- âœ… Valid records â†’ Delta tables (MinIO)  
- âš ï¸ Invalid records â†’ DLQ Delta table (MinIO)  

Handles:

- Partitioning  
- Table creation  
- Upserts / append logic  

---

## ğŸ§  Processing Flow

1. Consume event from Kafka  
2. Parse using lightweight schema  
3. If valid â†’ transform & write to Delta  
4. If invalid â†’ write to DLQ  

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Configure Environment

#### Update configs in `config/`

### 2ï¸âƒ£ Run Stream Job

```bash
export PYTHONPATH=$(pwd)/src
python -m stream_processor.main
```

---


